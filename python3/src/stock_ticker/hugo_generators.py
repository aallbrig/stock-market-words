"""
Hugo site content generators.
Generate data files and markdown pages for the Hugo site based on SQLite database.
"""
import json
import csv
import pandas as pd
from pathlib import Path
from datetime import datetime
from .config import DB_PATH, BASE_DIR, TMP_DIR
from .database import get_connection
from .utils import get_today
from .logging_setup import setup_logging

logger = setup_logging()

# Hugo site paths
HUGO_SITE_DIR = BASE_DIR / "hugo" / "site"
HUGO_DATA_DIR = HUGO_SITE_DIR / "data"  # Hugo's data directory for templates
HUGO_STATIC_DATA_DIR = HUGO_SITE_DIR / "static" / "data"  # For JavaScript access
HUGO_CONTENT_DIR = HUGO_SITE_DIR / "content"


def ensure_hugo_dirs():
    """Ensure Hugo directories exist."""
    HUGO_DATA_DIR.mkdir(parents=True, exist_ok=True)
    HUGO_STATIC_DATA_DIR.mkdir(parents=True, exist_ok=True)
    HUGO_CONTENT_DIR.mkdir(parents=True, exist_ok=True)


def generate_raw_ftp_data(dry_run=False):
    """
    Generate JSON/CSV files showing raw FTP data BEFORE filtering.
    
    Uses the last downloaded FTP files from TMP_DIR to show what came from NASDAQ.
    Includes table headers and all rows.
    """
    if dry_run:
        logger.info("DRY RUN: Would generate raw FTP data files")
        logger.info(f"DRY RUN: Output to {HUGO_DATA_DIR}")
        logger.info("DRY RUN: Files: raw_nasdaq.json, raw_otherlisted.json")
        return
    
    logger.info("=== Generating Raw FTP Data ===")
    ensure_hugo_dirs()
    
    nasdaq_file = TMP_DIR / "nasdaqlisted.txt"
    other_file = TMP_DIR / "otherlisted.txt"
    
    if not nasdaq_file.exists() or not other_file.exists():
        logger.warning("FTP files not found in tmp/. Run sync-ftp first.")
        return
    
    # Parse NASDAQ file
    logger.info("Processing nasdaqlisted.txt...")
    try:
        df_nasdaq = pd.read_csv(nasdaq_file, sep='|')
        # Remove footer row (contains "File Creation Time")
        df_nasdaq = df_nasdaq[~df_nasdaq['Symbol'].str.contains('File Creation Time', na=False)]
        # Remove any rows with NaN in critical columns
        df_nasdaq = df_nasdaq.dropna(subset=['Symbol', 'Security Name'])
        
        # Convert to records, replacing NaN with None for valid JSON
        records = df_nasdaq.to_dict('records')
        # Clean NaN values
        for record in records:
            for key, value in record.items():
                if pd.isna(value):
                    record[key] = None
        
        nasdaq_data = {
            '_meta': {
                'auto_generated': True,
                'generated_by': 'python3/src/stock_ticker/hugo_generators.py :: generate_raw_ftp_data()',
                'generated_at': datetime.now().isoformat(),
                'warning': 'DO NOT EDIT - This file is automatically generated and will be overwritten'
            },
            'source': 'NASDAQ FTP',
            'file': 'nasdaqlisted.txt',
            'downloaded_at': datetime.now().isoformat(),
            'total_rows': len(df_nasdaq),
            'columns': list(df_nasdaq.columns),
            'data': records
        }
        
        # Write to Hugo data directory (for templates)
        output_path = HUGO_DATA_DIR / "raw_nasdaq.json"
        with open(output_path, 'w') as f:
            json.dump(nasdaq_data, f, indent=2)
        
        # Also write to static directory (for JavaScript)
        static_path = HUGO_STATIC_DATA_DIR / "raw_nasdaq.json"
        with open(static_path, 'w') as f:
            json.dump(nasdaq_data, f, indent=2)
        
        logger.info(f"‚úì Written: {output_path} ({len(df_nasdaq)} rows)")
        
    except Exception as e:
        logger.error(f"Failed to process nasdaqlisted.txt: {e}")
    
    # Parse otherlisted file
    logger.info("Processing otherlisted.txt...")
    try:
        df_other = pd.read_csv(other_file, sep='|')
        # Remove footer row (contains "File Creation Time")
        df_other = df_other[~df_other['ACT Symbol'].str.contains('File Creation Time', na=False)]
        # Remove any rows with NaN in critical columns
        df_other = df_other.dropna(subset=['ACT Symbol', 'Security Name'])
        
        # Convert to records, replacing NaN with None for valid JSON
        records = df_other.to_dict('records')
        # Clean NaN values
        for record in records:
            for key, value in record.items():
                if pd.isna(value):
                    record[key] = None
        
        other_data = {
            '_meta': {
                'auto_generated': True,
                'generated_by': 'python3/src/stock_ticker/hugo_generators.py :: generate_raw_ftp_data()',
                'generated_at': datetime.now().isoformat(),
                'warning': 'DO NOT EDIT - This file is automatically generated and will be overwritten'
            },
            'source': 'NASDAQ FTP',
            'file': 'otherlisted.txt',
            'downloaded_at': datetime.now().isoformat(),
            'total_rows': len(df_other),
            'columns': list(df_other.columns),
            'data': records
        }
        
        # Write to Hugo data directory (for templates)
        output_path = HUGO_DATA_DIR / "raw_otherlisted.json"
        with open(output_path, 'w') as f:
            json.dump(other_data, f, indent=2)
        
        # Also write to static directory (for JavaScript)
        static_path = HUGO_STATIC_DATA_DIR / "raw_otherlisted.json"
        with open(static_path, 'w') as f:
            json.dump(other_data, f, indent=2)
        
        logger.info(f"‚úì Written: {output_path} ({len(df_other)} rows)")
        
    except Exception as e:
        logger.error(f"Failed to process otherlisted.txt: {e}")
    
    logger.info("‚úì Raw FTP data generation complete")


def generate_filtered_data(dry_run=False):
    """
    Generate JSON files showing filtered data AFTER Pass 1 (price extraction).
    
    Shows:
    - All tickers that passed filtering and are in the database
    - Their current price/volume data (if available)
    - Filtering statistics
    """
    if dry_run:
        logger.info("DRY RUN: Would generate filtered ticker data")
        logger.info(f"DRY RUN: Output to {HUGO_DATA_DIR}")
        logger.info("DRY RUN: Files: filtered_tickers.json, pass1_results.json")
        return
    
    logger.info("=== Generating Filtered Data ===")
    ensure_hugo_dirs()
    
    conn = get_connection()
    cursor = conn.cursor()
    
    # Get the most recent date with data (don't rely on today's date)
    cursor.execute("SELECT MAX(date) FROM daily_metrics WHERE price IS NOT NULL")
    result = cursor.fetchone()
    if not result or not result[0]:
        logger.warning("No data found in daily_metrics. Run 'ticker-cli run-all' first.")
        conn.close()
        return
    
    latest_date = result[0]
    logger.info(f"Using latest data from: {latest_date}")
    
    # Get all tickers in database (these passed filtering)
    logger.info("Querying filtered tickers...")
    cursor.execute("""
        SELECT 
            t.symbol,
            t.name,
            t.exchange,
            t.is_etf,
            t.first_seen,
            dm.date as price_date,
            dm.price,
            dm.volume,
            dm.market_cap,
            dm.dividend_yield,
            dm.beta,
            dm.rsi_14,
            dm.ma_200
        FROM tickers t
        LEFT JOIN daily_metrics dm ON t.symbol = dm.symbol AND dm.date = ?
        ORDER BY t.exchange, t.symbol
    """, (latest_date,))
    
    rows = cursor.fetchall()
    
    # Build filtered data structure
    filtered_data = {
        '_meta': {
            'auto_generated': True,
            'generated_by': 'python3/src/stock_ticker/hugo_generators.py :: generate_filtered_data()',
            'generated_at': datetime.now().isoformat(),
            'warning': 'DO NOT EDIT - This file is automatically generated and will be overwritten'
        },
        'generated_at': datetime.now().isoformat(),
        'date': latest_date,
        'total_tickers': len(rows),
        'exchanges': {},
        'tickers': []
    }
    
    exchange_counts = {}
    
    for row in rows:
        ticker_data = {
            'symbol': row[0],
            'name': row[1],
            'exchange': row[2],
            'is_etf': bool(row[3]),
            'first_seen': row[4],
            'price_data': {
                'date': row[5],
                'price': row[6],
                'volume': row[7],
                'market_cap': row[8],
                'dividend_yield': row[9],
                'beta': row[10],
                'rsi_14': row[11],
                'ma_200': row[12]
            } if row[5] else None
        }
        
        filtered_data['tickers'].append(ticker_data)
        
        # Count by exchange
        exchange = row[2]
        exchange_counts[exchange] = exchange_counts.get(exchange, 0) + 1
    
    filtered_data['exchanges'] = exchange_counts
    
    # Get filtering statistics
    cursor.execute("SELECT COUNT(*) FROM tickers")
    total_filtered = cursor.fetchone()[0]
    
    cursor.execute("SELECT COUNT(*) FROM daily_metrics WHERE date = ? AND price IS NOT NULL", (latest_date,))
    with_price = cursor.fetchone()[0]
    
    cursor.execute("SELECT COUNT(*) FROM daily_metrics WHERE date = ? AND market_cap IS NOT NULL", (latest_date,))
    with_metadata = cursor.fetchone()[0]
    
    filtered_data['statistics'] = {
        'total_tickers_in_db': total_filtered,
        'with_price_data': with_price,
        'with_metadata': with_metadata,
        'by_exchange': exchange_counts
    }
    
    conn.close()
    
    # Write filtered tickers to both locations
    output_path = HUGO_DATA_DIR / "filtered_tickers.json"
    with open(output_path, 'w') as f:
        json.dump(filtered_data, f, indent=2, default=str)
    
    static_path = HUGO_STATIC_DATA_DIR / "filtered_tickers.json"
    with open(static_path, 'w') as f:
        json.dump(filtered_data, f, indent=2, default=str)
    
    logger.info(f"‚úì Written: {output_path} ({len(rows)} tickers)")
    
    # Generate Pass 1 results summary
    pass1_data = {
        '_meta': {
            'auto_generated': True,
            'generated_by': 'python3/src/stock_ticker/hugo_generators.py :: generate_filtered_data()',
            'generated_at': datetime.now().isoformat(),
            'warning': 'DO NOT EDIT - This file is automatically generated and will be overwritten'
        },
        'generated_at': datetime.now().isoformat(),
        'date': latest_date,
        'pass1_extraction': {
            'total_attempted': total_filtered,
            'successful': with_price,
            'failed': total_filtered - with_price,
            'success_rate': f"{(with_price / total_filtered * 100):.1f}%" if total_filtered > 0 else "0%"
        },
        'pass2_extraction': {
            'total_attempted': with_price,
            'successful': with_metadata,
            'pending': with_price - with_metadata
        }
    }
    
    output_path = HUGO_DATA_DIR / "pass1_results.json"
    with open(output_path, 'w') as f:
        json.dump(pass1_data, f, indent=2)
    
    static_path = HUGO_STATIC_DATA_DIR / "pass1_results.json"
    with open(static_path, 'w') as f:
        json.dump(pass1_data, f, indent=2)
    
    logger.info(f"‚úì Written: {output_path}")
    logger.info("‚úì Filtered data generation complete")


def generate_hugo_pages(dry_run=False):
    """
    Generate Hugo markdown pages for data listings.
    
    Generates:
    - raw-ftp-data.md: Raw ticker data from NASDAQ FTP before filtering
    - filtered-data.md: Ticker data after filtering and price extraction
    
    Does NOT generate:
    - strategy-*.md: These pages are MANUALLY MAINTAINED in hugo/site/content/
      Strategy DATA (JSON files) is generated separately by generate_strategy_filters().
      This separation ensures manually written content (descriptions, usage tips, etc.)
      is never overwritten by automated processes.
    """
    if dry_run:
        logger.info("DRY RUN: Would generate Hugo markdown pages")
        logger.info(f"DRY RUN: Output to {HUGO_CONTENT_DIR}")
        return
    
    logger.info("=== Generating Hugo Pages ===")
    ensure_hugo_dirs()
    
    # Page 1: Raw FTP Data
    raw_ftp_page = """---
title: "Raw FTP Data"
description: "Raw ticker data from NASDAQ FTP before any filtering"
date: {}
type: "page"
layout: "raw-data"
---

<!-- 
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  ‚ö†Ô∏è  AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  
  This file is automatically generated by:
    python3/src/stock_ticker/hugo_generators.py :: generate_hugo_pages()
  
  Generated: {}
  
  To modify this content:
    1. Edit the template in hugo_generators.py
    2. Run: ticker-cli hugo-pages
  
  Manual edits will be overwritten on next generation.
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
-->

This page shows the raw ticker data downloaded from NASDAQ FTP servers **before** any filtering is applied.

## Data Sources

- **nasdaqlisted.txt**: All NASDAQ-listed securities
- **otherlisted.txt**: Securities listed on other exchanges (NYSE, AMEX, etc.)

The data includes test tickers, ETFs, warrants, units, preferred stock, and all other security types.

""".format(datetime.now().isoformat(), datetime.now().strftime("%Y-%m-%d %H:%M:%S UTC"))
    
    output_path = HUGO_CONTENT_DIR / "raw-ftp-data.md"
    with open(output_path, 'w') as f:
        f.write(raw_ftp_page)
    logger.info(f"‚úì Written: {output_path}")
    
    # Page 2: Filtered Data
    filtered_page = """---
title: "Filtered Ticker Data"
description: "Ticker data after Pass 1 filtering and price extraction"
date: {}
type: "page"
layout: "filtered-data"
---

<!-- 
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  ‚ö†Ô∏è  AUTO-GENERATED FILE - DO NOT EDIT MANUALLY
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  
  This file is automatically generated by:
    python3/src/stock_ticker/hugo_generators.py :: generate_hugo_pages()
  
  Generated: {}
  
  To modify this content:
    1. Edit the template in hugo_generators.py
    2. Run: ticker-cli hugo-pages
  
  Manual edits will be overwritten on next generation.
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
-->

This page shows the ticker data **after** filtering has been applied.

## Filtering Pipeline

1. **Test Issue Filter**: Removes test tickers (ZZZZ, TEST, etc.)
2. **ETF Filter**: Removes ETFs (common stocks only)
3. **Financial Status Filter**: Removes bankrupt/deficient tickers
4. **Keyword Filter**: Removes Units, Warrants, Rights, Preferred Stock
5. **Symbol Validation**: Removes invalid symbols and suffixes

## Pass 1: Price Extraction

After filtering, price/volume data is extracted from Yahoo Finance for all remaining tickers.

""".format(datetime.now().isoformat(), datetime.now().strftime("%Y-%m-%d %H:%M:%S UTC"))
    
    output_path = HUGO_CONTENT_DIR / "filtered-data.md"
    with open(output_path, 'w') as f:
        f.write(filtered_page)
    logger.info(f"‚úì Written: {output_path}")
    
    # ============================================================================
    # STRATEGY PAGES ARE MANUALLY MAINTAINED - DO NOT AUTO-GENERATE
    # ============================================================================
    # Strategy pages (strategy-*.md) contain rich, manually written content
    # including detailed descriptions, plain English explanations, usage tips,
    # and risk warnings. These pages are maintained by developers in:
    #   hugo/site/content/strategy-*.md
    #
    # Strategy DATA (ticker lists, scores) is auto-generated as JSON files by
    # the generate_strategy_filters() function and stored in:
    #   hugo/site/static/data/strategy_*.json
    #
    # To add a new strategy:
    #   1. Copy an existing strategy-*.md file as a template
    #   2. Update the front matter (title, description, strategy_key)
    #   3. Write the content (what is this strategy, filter criteria, usage tips)
    #   4. Add the strategy logic to generate_strategy_filters()
    # ============================================================================
    
    logger.info("‚ÑπÔ∏è  Strategy pages (strategy-*.md) are manually maintained")
    logger.info("‚úì Hugo page generation complete")


def generate_strategy_filters(dry_run=False):
    """
    Generate JSON files for each strategy showing which tickers match the filter criteria.
    
    Strategy filters:
    - Dividend Daddy: Must have dividend_yield > 0 (pays dividends)
    - Moon Shot: High beta (beta > 1.5) and not overbought (rsi < 70)
    - Falling Knife: Oversold (rsi < 30) and below 200-day MA
    - Over Hyped: Overbought (rsi > 70)
    - Institutional Whale: Large cap (market_cap > 10 billion)
    """
    if dry_run:
        logger.info("DRY RUN: Would generate strategy filter data")
        logger.info(f"DRY RUN: Output to {HUGO_DATA_DIR}")
        return
    
    logger.info("=== Generating Strategy Filter Data ===")
    ensure_hugo_dirs()
    
    conn = get_connection()
    cursor = conn.cursor()
    
    # Get the most recent date with data
    cursor.execute("SELECT MAX(date) FROM daily_metrics WHERE market_cap IS NOT NULL")
    result = cursor.fetchone()
    if not result or not result[0]:
        logger.warning("No data found in daily_metrics. Run 'ticker-cli run-all' first.")
        conn.close()
        return
    
    latest_date = result[0]
    logger.info(f"Using latest data from: {latest_date}")
    
    # Define strategy filters with SQL conditions
    strategies = {
        'dividend_daddy': {
            'name': 'Dividend Daddy',
            'description': 'Stocks that pay dividends (dividend yield > 0%)',
            'icon': 'üí∞',
            'sql_filter': 'dividend_yield > 0',
            'order_by': 'dividend_yield DESC'
        },
        'moon_shot': {
            'name': 'Moon Shot',
            'description': 'High volatility growth stocks (beta > 1.5, RSI < 70)',
            'icon': 'üöÄ',
            'sql_filter': 'beta > 1.5 AND rsi_14 < 70',
            'order_by': 'beta DESC'
        },
        'falling_knife': {
            'name': 'Falling Knife',
            'description': 'Oversold stocks below 200-day moving average (RSI < 30, price < MA200)',
            'icon': 'üî™',
            'sql_filter': 'rsi_14 < 30 AND ma_200 IS NOT NULL AND price < ma_200',
            'order_by': 'rsi_14 ASC'
        },
        'over_hyped': {
            'name': 'Over Hyped',
            'description': 'Overbought stocks (RSI > 70)',
            'icon': 'üìà',
            'sql_filter': 'rsi_14 > 70',
            'order_by': 'rsi_14 DESC'
        },
        'institutional_whale': {
            'name': 'Institutional Whale',
            'description': 'Large cap stocks (market cap > $10 billion)',
            'icon': 'üêã',
            'sql_filter': 'market_cap > 10000000000',
            'order_by': 'market_cap DESC'
        }
    }
    
    # Generate data for each strategy
    for strategy_key, strategy_info in strategies.items():
        logger.info(f"Processing {strategy_info['name']}...")
        
        query = f"""
            SELECT DISTINCT
                t.symbol,
                t.name,
                t.exchange,
                dm.price,
                dm.volume,
                dm.market_cap,
                dm.dividend_yield,
                dm.beta,
                dm.rsi_14,
                dm.ma_200,
                ss.dividend_daddy_score,
                ss.moon_shot_score,
                ss.falling_knife_score,
                ss.over_hyped_score,
                ss.inst_whale_score
            FROM tickers t
            JOIN daily_metrics dm ON t.symbol = dm.symbol
            LEFT JOIN strategy_scores ss ON t.symbol = ss.symbol AND ss.date = dm.date
            WHERE dm.date = ?
            AND dm.price >= 5.0
            AND dm.volume >= 100000
            AND {strategy_info['sql_filter']}
            ORDER BY {strategy_info['order_by']}
        """
        
        cursor.execute(query, (latest_date,))
        rows = cursor.fetchall()
        
        # Build ticker list
        tickers = []
        for row in rows:
            ticker_data = {
                'symbol': row[0],
                'name': row[1],
                'exchange': row[2],
                'price': round(float(row[3]), 2),
                'volume': int(row[4]),
                'marketCap': int(row[5]) if row[5] else None,
                'dividendYield': round(float(row[6] * 100), 2) if row[6] else None,
                'beta': round(float(row[7]), 2) if row[7] else None,
                'rsi': round(float(row[8]), 1) if row[8] else None,
                'ma200': round(float(row[9]), 2) if row[9] else None,
                'scores': {
                    'dividendDaddy': int(row[10]) if row[10] else None,
                    'moonShot': int(row[11]) if row[11] else None,
                    'fallingKnife': int(row[12]) if row[12] else None,
                    'overHyped': int(row[13]) if row[13] else None,
                    'instWhale': int(row[14]) if row[14] else None
                }
            }
            tickers.append(ticker_data)
        
        # Build output data structure
        strategy_data = {
            '_meta': {
                'auto_generated': True,
                'generated_by': 'python3/src/stock_ticker/hugo_generators.py :: generate_strategy_filters()',
                'generated_at': datetime.now().isoformat(),
                'warning': 'DO NOT EDIT - This file is automatically generated and will be overwritten'
            },
            'strategy': strategy_info['name'],
            'strategy_key': strategy_key,
            'description': strategy_info['description'],
            'icon': strategy_info['icon'],
            'generated_at': datetime.now().isoformat(),
            'date': latest_date,
            'total_matches': len(tickers),
            'filter_criteria': strategy_info['sql_filter'],
            'tickers': tickers
        }
        
        # Write to Hugo data directory (for templates)
        output_path = HUGO_DATA_DIR / f"strategy_{strategy_key}.json"
        with open(output_path, 'w') as f:
            json.dump(strategy_data, f, indent=2)
        
        # Also write to static directory (for JavaScript)
        static_path = HUGO_STATIC_DATA_DIR / f"strategy_{strategy_key}.json"
        with open(static_path, 'w') as f:
            json.dump(strategy_data, f, indent=2)
        
        logger.info(f"‚úì {strategy_info['name']}: {len(tickers)} tickers match")
    
    conn.close()
    logger.info("‚úì Strategy filter data generation complete")


def generate_all_hugo_content(dry_run=False):
    """Generate all Hugo site content."""
    if dry_run:
        logger.info("DRY RUN: Would generate all Hugo content")
        logger.info("DRY RUN: - Raw FTP data")
        logger.info("DRY RUN: - Filtered ticker data")
        logger.info("DRY RUN: - Strategy filter data")
        logger.info("DRY RUN: - Hugo markdown pages")
        return
    
    logger.info("======================================================================")
    logger.info("=== üìù GENERATING HUGO SITE CONTENT ===")
    logger.info("======================================================================")
    logger.info("")
    
    # Record step start
    from .database import record_pipeline_step
    record_pipeline_step('generate-hugo', 0, 'in_progress', dry_run=False)
    
    # Generate raw FTP data
    logger.info("Step 1: Generating raw FTP data...")
    generate_raw_ftp_data(dry_run=False)
    logger.info("")
    
    # Generate filtered data
    logger.info("Step 2: Generating filtered ticker data...")
    generate_filtered_data(dry_run=False)
    logger.info("")
    
    # Generate strategy filters
    logger.info("Step 3: Generating strategy filter data...")
    generate_strategy_filters(dry_run=False)
    logger.info("")
    
    # Generate Hugo pages
    logger.info("Step 4: Generating Hugo markdown pages...")
    generate_hugo_pages(dry_run=False)
    logger.info("")
    
    # Count generated files
    pages_generated = 7  # raw-ftp, filtered, 5 strategies
    
    # Record completion
    record_pipeline_step('generate-hugo', pages_generated, 'completed', dry_run=False)
    
    logger.info("======================================================================")
    logger.info("=== ‚úÖ HUGO CONTENT GENERATION COMPLETE ===")
    logger.info("======================================================================")
    logger.info(f"Content location: {HUGO_SITE_DIR}")
    logger.info(f"Data files: {HUGO_DATA_DIR}")
    logger.info(f"Pages: {HUGO_CONTENT_DIR}")
